# -*- coding: utf-8 -*-
"""Semantic.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GDj9S2HbEzQtFqlgShytD305WNeDPQWV
"""

from google.colab import drive
drive.mount('/content/drive/')

# custom_nlp and import libraries 
import gensim
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from gensim.models import Word2Vec
from sklearn.cluster import KMeans
import os
import re 
import spacy
from gensim.utils import simple_preprocess
from gensim import corpora
custom_nlp=spacy.load('en_core_web_sm')

#Tag remover funtion
def remove(txt,doc):
  try:
    doc=doc.replace(txt,'')
  except:
    txt="\\"+txt
    doc=doc.replace(txt,'')
  return doc

def remove_unwanted_extra(removeable_text_list,doc):
  try:
    for txt in removeable_text_list:
      doc=remove(txt,doc)
  except Exception as e:
    print(e)
    return 
  return doc

# lemmatization 
def lemmatize(doc):
  return list(set(token.lemma_ for  token in doc))
# remove Unwanted things from nlp object
def remove_unwanted(doc):
  words=list(set(token for token in doc if not token.is_stop and len(token.text)>2 and not token.is_space and not token.is_punct and not token.like_url and not token.like_email and any([str(i) in ['a','e','i','o','u'] for i in token.text]) and len(token.text)>1))
  return lemmatize(words)
# file line iterator 
def file_line(df):
  for line,cat in zip(df['text'],df['sentiment']):
    yield line.lower(),cat

# training data
path='/content/drive/My Drive/Company Workspace/C_dataset/Sentiment_Dataset .csv'
df=pd.read_csv(path,nrows=1000)
un_lst=['\t','\n' ,'<br>','<br />']
counter=0
for line,cat in file_line(df):
  df.loc[counter]=[' '.join(set(remove_unwanted(custom_nlp( re.sub(r'[\d.,/\;:{@#$%^&*}}!`~?\+\_\-\=(\[\]\'\"\()]',' ',remove_unwanted_extra(un_lst,line)) )))),cat ]
  counter+=1
  print(counter)

# convert csv file into 2d list of words
def prepare(df):
   for txt in df['text']:
     yield txt
# training data
# df=pd.read_csv(input('Enter the path of  preprocessed csv file: '))
sentences=[simple_preprocess(txt) for txt in prepare(df)]

# training model
word2vec_model = Word2Vec(sentences,window=3,workers=4, min_count=2)
word2vec_model.train(sentences,total_examples=len(sentences),epochs=300)
# word2vec_model=joblib.load('/content/drive/My Drive/word2vec_model.pkl') 
model_vocabulary=word2vec_model.wv.vocab

# transform words into vectors 
def transform(sentences,word2vec_model):
  Xtrain,special,general_vocabulary=[],[],[]
  for sentence in sentences:
    vec=[]
    for word in sentence:
      try:
        vec.append(word2vec_model.wv.get_vector(word).mean() )  
        general_vocabulary.append(word)
      except KeyError:
        special.append(word)
        print(f'special word found %s'%(word))
    vec=np.array(vec)
    vec.resize(144)
    Xtrain.append(vec)
  return Xtrain,special,general_vocabulary
Xtrain,special,general_vocabulary=transform(sentences,word2vec_model)

NUM_CLUSTERS=5
kclusterer = KMeansClusterer(NUM_CLUSTERS, distance=nltk.cluster.util.cosine_distance, repeats=50)
kmeans_model = cluster.KMeans(n_clusters=NUM_CLUSTERS)
kmeans_model.fit(Xtrain)
# joblib.dump(kmeans_model,'/content/drive/My Drive/kmeans_model.pkl')
dbscan=DBSCAN(eps=5,min_samples=2)    # DBSCAN model 
dbscan_pred=dbscan.fit_predict(Xtrain,labels)  # dbscan doesn't have predict method 
# kmeans_model= joblib.load('/content/drive/My Drive/kmeans_model.pkl') 
labels = kmeans_model.labels_
centroids = kmeans_model.cluster_centers_

print(model_vocabulary,special,general_vocabulary)





