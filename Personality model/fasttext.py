# -*- coding: utf-8 -*-
"""FASTTEXT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1b7hp6dHMc6vSjUX3NF_nQOiJdLfV1yyl
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install --upgrade gensim

"""# Downloading and unzipping fasttext pretrained model file"""

# Download dan unzip model
!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz
# !gunzip -k cc.en.300.bin.gz
# Install / Upgrade Gensim
# Load model method 1 
from gensim.models.fasttext import FastText, load_facebook_vectors
model = load_facebook_vectors("cc.en.300.bin.gz")
# Load model method 2 
# from gensim.models.fasttext import FastText
# model = FastText.load_fasttext_format('/content/drive/My Drive/cc.en.300.bin')

!pip install fasttext
import fasttext
# custom_nlp and import libraries 
import gensim
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from gensim.models import Word2Vec
from sklearn.cluster import KMeans
import os
import re 
import spacy
from gensim.utils import simple_preprocess
from gensim import corpora
custom_nlp=spacy.load('en_core_web_sm') # spacy nlp_pretrained model object 

from nltk.cluster import KMeansClusterer
import nltk
from sklearn import cluster
from sklearn import metrics
from sklearn.externals import joblib

# Removing tags from text like \n,',"
def remove(txt,doc):
  try:
    doc=doc.replace(txt,' ')
  except:
    txt="\\"+txt
    doc=doc.replace(txt,' ')
  return doc
# removing custom stopwords(given list by user) from tex
def remove_unwanted_extra(removeable_text_list,doc):
  try:
    for txt in removeable_text_list:
      doc=remove(txt,doc)
  except Exception as e:
    print(e)
    return 
  return doc

# lemmatization 
def lemmatize(doc):
  return list(set(token.lemma_ for  token in doc))
# remove Unwanted things from nlp object
def remove_unwanted(doc):
  words=list(set(token for token in doc if not token.is_stop and len(token.text)>2 and not token.is_space and not token.text.isdigit() and not token.is_punct and not token.is_bracket and not token.is_quote and not token.like_url and not token.like_num and not token.like_email and any([str(i) in ['a','e','i','o','u'] for i in token.text]) ))
  return lemmatize(words)
# file line iterator 
def file_line(path):
  for line in open(path,'r',encoding='utf-8').readlines():
    yield line.lower()

un_lst=['\t','\\','.','"','?',',','//','(',')','[',']','{','}','@','$','/' ]

#preprocessing text file

# path='/content/drive/My Drive/text_file.txt'
# text_file=open(path,'w')
# counter=0
# def get_extracted_text(path): 
#   global text_file
#   counter=0
#   for file in os.listdir(path):
#     counter+=1
#     print("File : counter : ",counter)
#     for line in file_line(os.path.join(path,file)):
#       text_file.write(' '+' '.join(set(remove_unwanted(custom_nlp( re.sub('\d','',remove_unwanted_extra(un_lst,line)) )) )))
# path='/content/drive/My Drive/Company Workspace/C_dataset/sample1/'   
# get_extracted_text(path)
# text_file.close()

# fasttext_model = fasttext.train_supervised(input='/content/drive/My Drive/text.txt', epoch=25, lr=1.0, wordNgrams=2, verbose=2, minCount=1)
word_vectors=gensim.models.KeyedVectors.load_word2vec_format('https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz', binary=True)
# fasttext_model=fasttext.train_unsupervised('/content/drive/My Drive/text_file.txt') 
# fasttext_model=fasttext.train_unsupervised('/content/drive/My Drive/text_file.txt',model='cbow',loss='softmax',lr=0.01,epoch=300,wordNgrams=2,verbose=1)

# def prepare(df):
#    for txt in df['text']:
#      yield txt
# training data
df=pd.read_csv('/content/drive/My Drive/data.csv')
sentences=[simple_preprocess(txt) for txt in df['text']]
def transform(sentences,word2vec_model):
        """ it takes 2d sequence of words to convert words into vectors
        Ex: [['hi','there'],['ok','you','got','it']]

        """
  Xtrain,special=[],[]
  for sentence in sentences:
    # vec=[]
    for word in sentence:
      try:
        # vec.append(word2vec_model.wv.word2vec(word) )
        vec=[vec for (word,vec) in word2vec_model.similar_by_word(word)]
      except KeyError:
        special.append(word)
        print(f'special word found %s'%(word))
    vec=np.array(vec)
    vec.resize(144)
    Xtrain.append(vec)
  return Xtrain,special
Xtrain,special=transform(sentences,word_vectors)

NUM_CLUSTERS=5
kclusterer = KMeansClusterer(NUM_CLUSTERS, distance=nltk.cluster.util.cosine_distance, repeats=50)
kmeans_model = cluster.KMeans(n_clusters=NUM_CLUSTERS)
kmeans_model.fit(Xtrain)
# kmeans_model= joblib.load('/content/drive/My Drive/kmeans_model.pkl') 
labels = kmeans_model.labels_
centroids = kmeans_model.cluster_centers_
# joblib.dump(kmeans_model,'/content/drive/My Drive/kmeans_model.pkl')

# cross validation checking Accuracy scores
from sklearn.model_selection import cross_val_score,KFold
kfold=KFold(n_splits=10,shuffle=True,random_state=1)
cross_validation_models=[kfold]
# for model in cross_validation_models:
print(f'********************************_Accuracy_scores_Of_%s***********************************'%model)
scores = cross_val_score(kmeans_model, Xtrain, labels, cv=kfold, scoring='accuracy')
print(np.array(scores).mean())

# joblib.dump(word_vectors,'/content/drive/My Drive/word2vec_model.pkl')
# joblib.dump(kmeans_model,'/content/drive/My Drive/kmeans_fastext_model.pkl')

# testing    Technical  :2
test=[set(remove_unwanted(custom_nlp( re.sub('\d','',remove_unwanted_extra(un_lst,input())) )) )] 
test,_=transform(test,word_vectors)
kmeans_model.predict(test)

# testing    sport  :1
test=[set(remove_unwanted(custom_nlp( re.sub('\d','',remove_unwanted_extra(un_lst,input())) )) )] 
test,_=transform(test,word_vectors)
kmeans_model.predict(test)

# testing    polytics :3
test=[set(remove_unwanted(custom_nlp( re.sub('\d','',remove_unwanted_extra(un_lst,input())) )) )] 
test,_=transform(test,word_vectors)
kmeans_model.predict(test)

# testing    Entertainment :4
test=[set(remove_unwanted(custom_nlp( re.sub('\d','',remove_unwanted_extra(un_lst,input())) )) )] 
test,_=transform(test,word_vectors)
kmeans_model.predict(test)

# testing   Business  :
test=[set(remove_unwanted(custom_nlp( re.sub('\d','',remove_unwanted_extra(un_lst,input())) )) )] 
test,_=transform(test,word_vectors)
kmeans_model.predict(test)





